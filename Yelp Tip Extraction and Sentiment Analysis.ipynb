{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp Tips Extraction and Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "- Restaurant goers often face the problem of picking a restaurant. They often go through multiple reviews to come to a decision. \n",
    "- The restaurant owners too must go through each and every long review to understand what the customers are thinking about their restaurants.\n",
    "- Our model extracts relevant tips from a large set of restaurant reviews and perform sentiment analysis on those tips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This scraper generates a list of files containing restaurant reviews. This scraper extracts reviews close to 10K \n",
    "from yelp.com\n",
    "'''\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "cuisine = input(\"Enter the cuisine: \")\n",
    "path = \"C:/Users/prath/Desktop/\"+cuisine+\"/\"\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    restaurant_name = filename\n",
    "    restaurant_name = restaurant_name.split(\"_review\",1)[0]\n",
    "    print(restaurant_name)\n",
    "\n",
    "    soup = None\n",
    "    for i in range(5):  # try 5 times\n",
    "        try:\n",
    "            soup = BeautifulSoup(open(path+filename, encoding=\"utf-8\"),\"html.parser\")\n",
    "            break  # we got the file, break the loop\n",
    "        except Exception as e:  # browser.open() threw an exception, the attempt to get the response failed\n",
    "            print('failed attempt', i)\n",
    "            time.sleep(2)  # wait 2 secs\n",
    "\n",
    "    if not soup: continue  # couldnt get the page, ignore\n",
    "\n",
    "    reviews = soup.findAll('div', {'class': 'review-content'})  # get all the review divs\n",
    "    for review in reviews:\n",
    "        text = review.find('p',{'lang': 'en'})\n",
    "        if text:\n",
    "            with open(restaurant_name+\".txt\",'a+') as f:\n",
    "                f.write(text.text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is a sample code for generating a list of Mexican Restaurants in New York (Contains URL identifiers)\n",
    "'''\n",
    "\n",
    "import unicodedata\n",
    "from lxml import etree\n",
    "import requests\n",
    "from lxml import html\n",
    "import numpy as np\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "root = etree.Element('html')\n",
    "root.tag\n",
    "etree.SubElement(root,'head')\n",
    "etree.SubElement(root,'body')\n",
    "print(etree.tostring(root))\n",
    "\n",
    "s=np.arange(0,1000,10)\n",
    "\n",
    "f = open(\"List_NY.txt\",\"w\")\n",
    "\n",
    "title=[]\n",
    "for j in s:    \n",
    "    page=requests.get('https://www.yelp.com/search?find_desc=Mexican+Food&find_loc=New+York,+NY&start='+str(j))\n",
    "    html_content = html.fromstring(page.content)\n",
    "    for i in range(3,11):\n",
    "        x=[]\n",
    "        x=html_content.xpath('//*[@id=\"super-container\"]/div/div[2]/div[1]/div/div[4]/ul[2]/li['+str(i)+']/div/div[1]/div[1]/div/div[2]/h3/span/a/@href')\n",
    "        y=str(x).replace(',','Mexican').replace(\"'\",\"\").replace('[','').replace(']','')\n",
    "        f.write(\"%s\\n\"%y)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This scraper takes the list of restaurants as input and scrapes 'review highlights' of all the restaurants\n",
    "'''\n",
    "\n",
    "from lxml import etree\n",
    "import requests\n",
    "from lxml import html\n",
    "import string\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "import nltk\n",
    "\n",
    "\n",
    "root = etree.Element('html')\n",
    "root.tag\n",
    "etree.SubElement(root,'head')\n",
    "etree.SubElement(root,'body')\n",
    "print(etree.tostring(root))\n",
    "\n",
    "path='List_NY.txt'    #Reading the file from which the restaurants names are taken to search in Yelp and Scrape the reviews\n",
    "fin=open(path)\n",
    "f = open(\"List_NY_highlights.txt\",\"w\")\n",
    "for line in fin:                                              \n",
    "                words = line.lower().strip()                     \n",
    "                restraunt_name=words\n",
    "                url='https://www.yelp.com'+restraunt_name\n",
    "                page= requests.get(url)\n",
    "                html_content = html.fromstring(page.content)\n",
    "                for i in range(1,4):\n",
    "                    x=[]\n",
    "                    y=html_content.xpath('//*[@id=\"super-container\"]/div/div/div[1]/div[1]/div[1]/ul/li['+str(i)+']/div[2]/p/a[1]/text()')\n",
    "                    z=html_content.xpath('//*[@id=\"super-container\"]/div/div/div[1]/div[1]/div[1]/ul/li['+str(i)+']/div[2]/p/text()')\n",
    "                    \n",
    "                    z=str(z).replace('[','').replace(']','')\n",
    "                    z=str(z).replace(\"', '\\n    '\",\"\")\n",
    "                    z=z.replace(\"'\", \"\")\n",
    "                    \n",
    "                    x = nltk.word_tokenize(z)\n",
    "                    x = [''.join(c for c in s if c not in string.punctuation) for s in x]\n",
    "                    x = [s for s in x if s]\n",
    "                    \n",
    "                    x1= list(filter(('n').__ne__, x))    \n",
    "                    size = len(x1)\n",
    "                    x1 = x1[1:size-2]\n",
    "                    detokenizer = MosesDetokenizer()\n",
    "                    list1=detokenizer.detokenize(x1, return_str=True)\n",
    "                    list1.strip()\n",
    "                    list1=list1[1:-1]\n",
    "                    f.write(\"%s\\n\"%list1)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The script includes the following pre-processing steps for text:\n",
    "- Sentence Splitting\n",
    "- Term Tokenization\n",
    "- Ngrams\n",
    "- POS tagging\n",
    "\n",
    "The run function includes all bigrams of the form: <ADVERB> <ADJECTIVE>\n",
    "'''\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import load\n",
    "\n",
    "def getAdvAdjTwograms(terms,adj,adv): # return all the 'adv adj' twograms\n",
    "    result=[]\n",
    "    twograms = ngrams(terms,2)  \n",
    "    for tg in twograms:  \n",
    "        if tg[0] in adv and tg[1] in adj: # if the 2gram is a an adverb followed by an adjective\n",
    "            result.append(tg)\n",
    "    return result\n",
    "\n",
    "def getAdjOnegrams(terms,adj): # return all the 'adj' unigrams\n",
    "    resultadj=[]\n",
    "    onegrams = ngrams(terms,1) #compute unigrams\n",
    "    for og in onegrams:  \n",
    "        if og[0] in adj: # if the unigram is a an adjective\n",
    "            resultadj.append(og[0])\n",
    "    return resultadj\n",
    "\n",
    "def getPOSterms(terms,POStags,tagger): # return all the terms that belong to a specific POS type\n",
    "    tagged_terms=tagger.tag(terms) # do POS tagging on the tokenized sentence\n",
    "    POSterms={}\n",
    "    for tag in POStags:POSterms[tag]=set()\n",
    "    for pair in tagged_terms:     #for each tagged term\n",
    "        for tag in POStags:     # for each POS tag \n",
    "            if pair[1].startswith(tag): POSterms[tag].add(pair[0])\n",
    "    return POSterms\n",
    "\n",
    "def run1(fpath):\n",
    "    _POS_TAGGER = 'taggers/maxent_treebank_pos_tagger/english.pickle'\n",
    "    tagger = load(_POS_TAGGER)\n",
    "    f=open(fpath)\n",
    "    text=f.read().strip()\n",
    "    f.close()\n",
    "    sentences=sent_tokenize(text)\n",
    "    print ('NUMBER OF SENTENCES: ',len(sentences))\n",
    "    adjAfterAdv=[]\n",
    "    for sentence in sentences:\n",
    "        terms = nltk.word_tokenize(sentence)   \n",
    "        POStags=['JJ','RB'] # POS tags of interest \n",
    "        POSterms=getPOSterms(terms,POStags,tagger)\n",
    "        adjectives=POSterms['JJ']\n",
    "        adverbs=POSterms['RB']\n",
    "        adjAfterAdv+=getAdvAdjTwograms(terms, adjectives, adverbs)\n",
    "        newadjAfterAdv=set(adjAfterAdv)\n",
    "    return newadjAfterAdv\n",
    "\n",
    "def run2(fpath):\n",
    "    _POS_TAGGER = 'taggers/maxent_treebank_pos_tagger/english.pickle'\n",
    "    tagger = load(_POS_TAGGER)\n",
    "    f=open(fpath)\n",
    "    text=f.read().strip()\n",
    "    f.close()\n",
    "    sentences=sent_tokenize(text)\n",
    "    print ('NUMBER OF SENTENCES: ',len(sentences))\n",
    "    adjAfterAdv=[]\n",
    "    for sentence in sentences:\n",
    "        terms = nltk.word_tokenize(sentence)   \n",
    "        POStags=['JJ','RB'] # POS tags of interest\n",
    "        POSterms=getPOSterms(terms,POStags,tagger)\n",
    "        adjectives=POSterms['JJ']\n",
    "        adjAfterAdv+=getAdjOnegrams(terms, adjectives)\n",
    "        newadjAfterAdv=set(adjAfterAdv)\t\n",
    "    return newadjAfterAdv\n",
    "\n",
    "if __name__=='__main__':\n",
    "    run1=run1('List_NY_highlights.txt')\n",
    "    run2=run2('List_NY_highlights.txt')\n",
    "    f = open(\"List_lexicon_of_expressions.txt\",\"w\")\n",
    "    for elem in run1:\n",
    "        for elemlist in elem:\n",
    "            f.write(str(elemlist)+\" \")\n",
    "        f.write(\"\\n\")\n",
    "    for elem in run2:\n",
    "        f.write(\"\\n\"+str(elem))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This step is used to extract tips from the main review file using the lexicon prepared in the above step\n",
    "'''\n",
    "import re\n",
    "\n",
    "\n",
    "f=open(\"tips.txt\",'w')\n",
    "def loadLexicon(fname):\n",
    "    newLex=set()\n",
    "    lex_conn=open(fname)\n",
    "    #add every word in the file to the set\n",
    "    for sentence in lex_conn:\n",
    "        newLex.add(sentence.strip())# remember to strip to remove the lin-change character\n",
    "    lex_conn.close()\n",
    "    return newLex\n",
    "\n",
    "searchfile = open(\"Italian/rosies-new-york.txt\")            #Restaurant review file\n",
    "file_lex=loadLexicon('List_lexicon_of_expressions.txt')\n",
    "text=searchfile.read().lower()\n",
    "sentences = text.split('.')\n",
    "\n",
    "for sent in sentences:\n",
    "    #print(sent)\n",
    "    sent=sent.lower().strip()\n",
    "    sent=re.sub(\"[^a-zA-Z0-9|?|.|,|!|-|:|;|&|@|_|/|>|<|#|$|']\",' ',sent)\n",
    "    words=sent.split(' ')\n",
    "    unique=set()\n",
    "    for word in words:\n",
    "        if word in file_lex:\n",
    "            unique.add(sent)\n",
    "    for i in unique:   \n",
    "        f.write(str(i)+\" \"+\"\\n\")\n",
    "f.close()\n",
    "searchfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "To further filter the review for users, we are using this code to create list of relavent noun words. This step will allow\n",
    "us to filter out only those tips that are relevant during the next step\n",
    "'''\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import load\n",
    "from textblob import TextBlob, Word\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "\n",
    "f=open('tips.txt')\n",
    "fn=open('nouns.txt','w')\n",
    "text=f.read().strip()\n",
    "f.close()\n",
    "blob = TextBlob(text)\n",
    "\n",
    "nouns = list()\n",
    "for word, tag in blob.tags:\n",
    "    if tag == 'NN':\n",
    "        nouns.append(word.lemmatize())\n",
    "nouns=[x for x in nouns if x != 'i']\n",
    "fn.write(str(nouns))\n",
    "freqNouns = Counter(nouns)\n",
    "with open('nouns.csv','w') as csvfile:\n",
    "    writer=csv.writer(csvfile,dialect='excel')\n",
    "    for noun in nouns:\n",
    "        if noun:\n",
    "            writer.writerow([noun])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This step uses word2vec model to find similar or relevant words to our word of interest. This sample code gives out a list \n",
    "of words relevant to the word 'food'\n",
    "'''\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import logging\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pylab import rcParams\n",
    "import csv\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "f=open(\"relevant.txt\",'w')\n",
    "\n",
    "# train word2vec model on the list\n",
    "model0 = Word2Vec(LineSentence(\"nouns.csv\"), size=10, window=2, min_count=1, workers=4)\n",
    "list1=model0.most_similar('food', topn=10)\n",
    "print((list1[0]))\n",
    "with open(\"relevant.csv\",\"w\") as result:\n",
    "    wr = csv.writer(result,dialect=\"excel\")\n",
    "    for each in list1:\n",
    "        if each:\n",
    "            wr.writerow(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This step generates a list of relevant tips based on the list of relevant words which will be used for recommendation \n",
    "for the customers and restaurant owner \n",
    "'''\n",
    "\n",
    "import unicodedata\n",
    "from lxml import etree\n",
    "import requests\n",
    "from lxml import html\n",
    "import numpy as np\n",
    "import io\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "g=open(\"final_tips.txt\",'w')\n",
    "file = open('nouns.csv')\n",
    "column_1st = pd.read_csv(file, sep=',',header=0,usecols=[0])\n",
    "listw=column_1st['words'].values.tolist()\n",
    "content = []\n",
    "with open(\"tips.txt\") as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "    content = [x.strip() for x in content]\n",
    "unique={}                             \n",
    "for sent in content:\n",
    "    for word in listw:\n",
    "        if word in sent:\n",
    "            unique[word] = sent\n",
    "                      \n",
    "for i in unique:\n",
    "    g.write(str(i)+\": \"+unique[i]+\"\\n\")  \n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This step performs sentiment analysis on the final tips\n",
    "'''\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize(str):\n",
    "        text = str.lower()\n",
    "        pattern = r'[a-z]+[a-z\\-\\.]*'\n",
    "        tokens = nltk.regexp_tokenize(text, pattern) \n",
    "        return tokens\n",
    "    \n",
    "class Text_Analyzer(object):\n",
    "    def _init_(self, input_file):\n",
    "        self.input = input_file\n",
    "\n",
    "    def sentiment_analysis(self):\n",
    "        with open(self.input) as f:\n",
    "            line=f.readlines()\n",
    "            line = [line.rstrip('\\n') for line in line] \n",
    "            for tip in line:\n",
    "                print(tip)\n",
    "                tokens = tokenize(str(tip))\n",
    "                stop_words = stopwords.words('english')\n",
    "                filtered_tokens=[token for token in tokens if token not in stop_words] \n",
    "                \n",
    "                with open(\"positive-words.txt\", 'r') as  value:\n",
    "                    positive_word = [line.strip() for line in value]\n",
    "                positive_tokens = [token for token in filtered_tokens if token in positive_word]\n",
    "                with open(\"negative-words.txt\", 'r') as  value:\n",
    "                    negative_word = [line.strip() for line in value] \n",
    "                negative_tokens = [token for token in filtered_tokens if token in negative_word]\n",
    "           \n",
    "                if len(positive_tokens)>len(negative_tokens):\n",
    "                    response = \"positive\"\n",
    "                if len(positive_tokens)<len(negative_tokens):\n",
    "                     response=\"negative\"\n",
    "                if len(positive_tokens)==len(negative_tokens):\n",
    "                     response=\"neutral\"    \n",
    "                print(\"The tip is:\",str(response))\n",
    "            return response\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    response = Text_Analyzer(\"final_tips.txt\")\n",
    "    sentiment= response.sentiment_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "- This model would efficiently assist restaurants in keeping up with good work they are doing, and improve upon the negatives. \n",
    "- The restaurant owners would specifically know what to improve upon rather than generic positive/negative reviews.\n",
    "- The customers too would be highly benefitted by the positive tips fed to the website, thus saving up on time of going   through the entire reviews and looking for what is good or bad in the restaurant. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
